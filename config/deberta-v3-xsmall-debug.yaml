model_name: &model_name deberta-v3-xsmall-japanese-debug

seed: &seed 42

dataset_names:
  - wikipedia

sentencepiece:
  model_type: unigram
  vocab_size: 32000
  character_coverage: 0.9995
  shuffle_input_sentence: true
  max_sentence_length: 8192
  split_by_unicode_script: false
  split_by_number: false
  split_by_whitespace: false
  split_digits: false
  allow_whitespace_only_pieces: true
  user_defined_symbols: ["[PAD]", "[CLS]", "[SEP]", "[MASK]"]
  normalization_rule_name: nmt_nfkc
  add_dummy_prefix: false
  pad_id: 0
  bos_id: 1
  eos_id: 2
  unk_id: 3
  pad_piece: "[PAD]"
  bos_piece: "[CLS]"
  eos_piece: "[SEP]"
  unk_piece: "[UNK]"

tokenizer:
  do_lower_case: false
  split_by_punct: false
  bos_token: "[CLS]"
  eos_token: "[SEP]"
  unk_token: "[UNK]"
  sep_token: "[SEP]"
  pad_token: "[PAD]"
  cls_token: "[CLS]"
  mask_token: "[MASK]"

data_collator:
  mlm_probability: 0.15

model:
  discriminator:
    attention_probs_dropout_prob: 0.1
    hidden_act: gelu
    hidden_dropout_prob: 0.1
    hidden_size: 384
    initializer_range: 0.02
    intermediate_size: 1536
    max_position_embeddings: 512
    relative_attention: true
    position_buckets: 256
    norm_rel_ebd: layer_norm
    share_att_key: true
    pos_att_type: p2c|c2p
    layer_norm_eps: 1.0e-7
    max_relative_positions: -1
    position_biased_input: false
    num_attention_heads: 6
    num_hidden_layers: 12
    type_vocab_size: 0
    vocab_size: 32000
  generator:
    attention_probs_dropout_prob: 0.0
    hidden_act: gelu
    hidden_dropout_prob: 0.0
    hidden_size: 384
    initializer_range: 0.02
    intermediate_size: 1536
    max_position_embeddings: 512
    relative_attention: true
    position_buckets: 256
    norm_rel_ebd: layer_norm
    share_att_key: true
    pos_att_type: p2c|c2p
    layer_norm_eps: 1.0e-7
    max_relative_positions: -1
    position_biased_input: false
    num_attention_heads: 6
    num_hidden_layers: 6
    type_vocab_size: 0
    vocab_size: 32000
  # torch_dtype: bfloat16

trainer:
  evaluation_strategy: steps
  per_device_train_batch_size: 64
  per_device_eval_batch_size: 64
  gradient_accumulation_steps: 32
  learning_rate: 6.0e-4
  weight_decay: 0.01
  adam_beta1: 0.9
  adam_beta2: 0.98
  adam_epsilon: 1.0e-6
  max_grad_norm: 1.0
  max_steps: 500
  lr_scheduler_type: linear
  warmup_steps: 10
  log_level: warning
  logging_strategy: steps
  logging_steps: 10
  save_strategy: steps
  save_steps: 100
  save_total_limit: 1
  seed: *seed
  data_seed: *seed
  bf16: true
  bf16_full_eval: false
  eval_steps: 100
  run_name: *model_name
  disable_tqdm: false
  load_best_model_at_end: true
  metric_for_best_model: loss
  greater_is_better: false
  optim: adamw_torch_fused
  group_by_length: true
  report_to: wandb
  gradient_checkpointing: false
  full_determinism: false
  torch_compile: true

max_length: 512
